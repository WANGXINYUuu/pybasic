{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 随机数种子\n",
    "随机种子（Random Seed）是用于初始化随机数生成器的一个固定值。通过设置相同的随机种子，随机数生成器会产生相同的随机数序列，这对于实验和结果的可重复性至关重要。\n",
    "常见的随机操作包括：\n",
    "+ 数据集的随机拆分(如训练集和测试集的划分)\n",
    "+ 模型参数的随机初始化\n",
    "+ 数据增强中的随机变换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of setting random seed for reproducibility\n",
    "# the results will be the same each time\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "seed = 66\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "# torch.manual_seed(seed)\n",
    "random_matrix_first = np.random.randn(3, 3)\n",
    "print(random_matrix_first)\n",
    "random_data_one_first = random.random()\n",
    "random_data_two_first = random.random()\n",
    "print(random_data_one_first, random_data_two_first)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of not setting random seed\n",
    "# the results will be different each time\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "random_matrix_second = np.random.randn(3, 3)\n",
    "print(random_matrix_second)\n",
    "random_data_one_second = random.random()\n",
    "random_data_two_second = random.random()\n",
    "print(random_data_one_second, random_data_two_second)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# argparser\n",
    "是python标准库中的一个模块，用于解析命令行参数。argparser模块使得编写用户友好的命令行界面变得容易。它可以解析命令行参数并生成帮助信息。argparser模块还可以自动生成帮助信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "import argparse\n",
    "# create the object of ArgumentParser\n",
    "parser = argparse.ArgumentParser(description = \"Description of the program\")\n",
    "# add_argument() method is used to add the arguments\n",
    "parser.add_argument(\"--arg1\", help = \"Description of the arg1\")\n",
    "parser.add_argument(\"--optional_arg\", default = \"default_value\", help = \"Description of the optional_arg\")\n",
    "# parse_args() method is used to parse the arguments\n",
    "args = parser.parse_args()\n",
    "# use the arguments\n",
    "print(args.arg1)\n",
    "print(args.optional_arg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About function \"DataLoader\" in torch.utils.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The parameter \"collate_fn\" of function \"DataLoader\" in torch.utils.data\n",
    "collate_fn：将一个batch的数据样本组织成一个batch，这个batch中的每个数据样本的形状可能不一样，所以需要用一个函数来组织这个batch。默认的collate_fn函数会将数据样本组织成一个list，这样就不会出现形状不一样的问题。如果数据样本的形状一样，可以不用设置collate_fn参数。\n",
    "\n",
    "作用：\n",
    "+ 自定义合并逻辑：当数据集中的样本结构复杂或者每个样本的形状不一致时，通过\"collate_fn\"自定义合并逻辑能够更灵活地处理这些样本。\n",
    "+ 处理变长序列：对于自然语言处理等任务，样本（例如句子）长度不一，需要自定义合并逻辑进行填充和打包。\n",
    "+ 处理多模态数据：当一个样本包含多种不同模态的数据（如图像和文本）时，需要自定义合并逻辑以正确处理和打包这些数据。\n",
    "\n",
    "使用方法：\n",
    "+ 定义一个函数，该函数接受一个batch的数据样本列表，返回一个batch的数据样本。'collate_fn'函数中的'batch'是一个列表，包含了多个通过'\\_\\_getitem_\\_'方法获取的数据样本。\n",
    "+ 将该函数传递给DataLoader的collate_fn参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "data = [\n",
    "    {'input': torch.tensor([1, 2, 3]), 'label': torch.tensor(0)},\n",
    "    {'input': torch.tensor([4, 5]), 'label': torch.tensor(1)},\n",
    "    {'input': torch.tensor([6, 7, 8, 9]), 'label': torch.tensor(2)}\n",
    "]\n",
    "\n",
    "dataset = CustomDataset(data)\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    inputs = [item['input'] for item in batch]\n",
    "    labels = [item['label'] for item in batch]\n",
    "\n",
    "    lengths = [len(item) for item in inputs]\n",
    "    max_length = max(lengths)\n",
    "\n",
    "    padded_inputs = torch.zeros((len(inputs), max_length))\n",
    "    for i, input in enumerate(inputs):\n",
    "        padded_inputs[i, :len(input)] = input\n",
    "    \n",
    "    labels = torch.stack(labels)\n",
    "\n",
    "    return padded_inputs, labels, lengths\n",
    "\"\"\"\n",
    "first batch:\n",
    "[\n",
    "    {'input': torch.tensor([1, 2, 3]), 'label': torch.tensor(0)},\n",
    "    {'input': torch.tensor([4, 5]), 'label': torch.tensor(1)}\n",
    "]\n",
    "\n",
    "second batch:\n",
    "[\n",
    "    {'input': torch.tensor([6, 7, 8, 9]), 'label': torch.tensor(2)}   \n",
    "]\n",
    "\"\"\"\n",
    "dataloader = DataLoader(dataset, batch_size = 2, shuffle = False, collate_fn = custom_collate_fn)\n",
    "\n",
    "for batch in dataloader:\n",
    "    padded_inputs, labels, lengths = batch\n",
    "    print(\"Padded inputs:\\n\", padded_inputs)\n",
    "    print(\"Labels:\\n\", labels)\n",
    "    print(\"Lengths:\\n\", lengths)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The parameter \"sampler\" and \"batch_sampler\" of function \"DataLoader\" in torch.utils.data\n",
    "sampler: \n",
    "\n",
    "定义如何从数据集中采样单个样本。它接受一个\"sampler\"对象。pytorch中提供了一些内置的采样器:torch.utils.data.RandomSampler, torch.utils.data.SequentialSampler, torch.utils.data.SubsetRandomSampler, torch.utils.data.WeightedRandomSampler等。这些采样器，都接收一个dataset对象，返回一个索引序列，用于获取数据集中的样本。\n",
    "\n",
    "batch_sampler: \n",
    "\n",
    "将单个样本采样器生成的样本索引打包成批次。它接受一个\"batch_sampler\"对象。pytorch中提供了一些内置的batch采样器:torch.utils.data.BatchSampler, torch.utils.data.WeightedRandomSampler等。这些batch采样器，接收一个sampler对象，返回一个索引序列，用于获取数据集中的样本。\n",
    "\n",
    "通过这两个参数，pytorch的'DataLoader'提供了灵活且强大的数据加载和采样机制，满足不同的实验和训练要求。另外可以，**自定义'sampler'和'batch_sampler'可以满足特定的数据采样需求**。\n",
    "\n",
    "自定义'sampler'：\n",
    "+ 继承torch.utils.data.Sampler类，实现\\_\\_iter\\_\\_和\\_\\_len\\_\\_方法。\n",
    "\n",
    "自定义'batch_sampler'：\n",
    "+ 继承torch.utils.data.BatchSampler类，实现\\_\\_iter\\_\\_和\\_\\_len\\_\\_方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example: sampler\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "\n",
    "data = torch.arange(10).view(-1, 1)\n",
    "dataset = TensorDataset(data)\n",
    "\n",
    "print(\"Example of sampler\")\n",
    "random_sampler = torch.utils.data.RandomSampler(dataset)\n",
    "dataloader_one = DataLoader(dataset, sampler = random_sampler, batch_size = 2)\n",
    "for batch in dataloader_one:\n",
    "    print(batch)\n",
    "\n",
    "print('-------------------')\n",
    "\n",
    "# example: batch_sampler\n",
    "print(\"Example of batch sampler\")\n",
    "batch_sampler = torch.utils.data.BatchSampler(random_sampler, batch_size = 3, drop_last = False)\n",
    "dataloader_two = DataLoader(dataset, batch_sampler = batch_sampler)\n",
    "for batch in dataloader_two:\n",
    "    print(batch)\n",
    "\n",
    "print('-------------------')\n",
    "\n",
    "# example: customize sampler\n",
    "class ReverseSampler(torch.utils.data.Sampler):\n",
    "    def __init__(self, data_source):\n",
    "        self.data_source = data_source\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return iter(range(len(self.data_source) - 1, -1, -1))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_source)\n",
    "\n",
    "print(\"Exampler of customized sampler\")\n",
    "reverse_sampler = ReverseSampler(dataset)\n",
    "dataloader_three = DataLoader(dataset, sampler = reverse_sampler, batch_size = 2) \n",
    "\n",
    "for batch in dataloader_three:\n",
    "    print(batch)\n",
    "\n",
    "print('-------------------')\n",
    "\n",
    "# example: customize batch sampler\n",
    "class CustomBatchSampler(torch.utils.data.BatchSampler):\n",
    "    def __init__(self, sampler, batch_size, drop_last):\n",
    "        self.sampler = sampler\n",
    "        self.batch_size = batch_size\n",
    "        self.drop_last = drop_last\n",
    "    \n",
    "    def __iter__(self):\n",
    "        batch = []\n",
    "        for index in self.sampler:\n",
    "            batch.append(index)\n",
    "            if len(batch) == self.batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        \n",
    "        if len(batch) > 0 and not self.drop_last:\n",
    "            yield batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.drop_last:\n",
    "            return len(self.sampler) // self.batch_size\n",
    "        else: \n",
    "            return (len(self.sampler) + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "batch_sampler = CustomBatchSampler(reverse_sampler, batch_size = 3, drop_last = False)\n",
    "dataloader_four = DataLoader(dataset, batch_sampler = batch_sampler)\n",
    "\n",
    "for batch in dataloader_four:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xavier 初始化\n",
    "Xavier初始化是用于初始化神经网络权重的一种方法，其目的是让信号能够更好地在网络层间传播，从而避免梯度消失或者爆炸的问题。\n",
    "\n",
    "Xavier初始化分为均匀分布(Xavier Uniform)和正态分布(Xavier Normal)两种方式。\n",
    "\n",
    "**Xavier Uniform**:\n",
    "\n",
    "W ~ U($-\\sqrt{\\frac{6}{n_{in}+n_{out}}}, \\sqrt{\\frac{6}{n_{in}+n_{out}}}$)\n",
    "+ U(a, b)表示均匀分布，a和b分别是分布的上下界。\n",
    "+ $n_{in}$是输入单元的数量，$n_{out}$是输出单元的数量。\n",
    "\n",
    "**Xavier Normal**:\n",
    "\n",
    "W ~ N(0, $\\sqrt{\\frac{2}{n_{in}+n_{out}}}$)\n",
    "+ N($\\mu$, $\\sigma^2$)表示正态分布，$\\mu$和$\\sigma^2$分别是分布的均值和方差。\n",
    "+ $n_{in}$是输入单元的数量，$n_{out}$是输出单元的数量。\n",
    "\n",
    "总结：\n",
    "+ Xavier Uniform：权重从均匀分布中采样，适用于浅层网络，能够提供平滑的信号传播\n",
    "+ Xavier Normal： 权重从正态分布中采样，适用于深层网络，能防止梯度消失或爆炸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.9657, -0.8036,  0.9522],\n",
      "        [ 0.2050,  0.8093,  0.1484]])\n",
      "Parameter containing:\n",
      "tensor([[-0.8619,  0.8579,  0.4230],\n",
      "        [-0.4476, -0.2066, -0.1763]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "# Example: Xavier Uniform Initialization\n",
    "linear = nn.Linear(3, 2)\n",
    "nn.init.xavier_uniform_(linear.weight).requires_grad_(False)\n",
    "print(linear.weight)\n",
    "\n",
    "# Example: Xavier Normal Initialization\n",
    "linear = nn.Linear(3, 2)\n",
    "nn.init.xavier_normal_(linear.weight).requires_grad_(False)\n",
    "print(linear.weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
