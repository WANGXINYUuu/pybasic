{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 随机数种子\n",
    "随机种子（Random Seed）是用于初始化随机数生成器的一个固定值。通过设置相同的随机种子，随机数生成器会产生相同的随机数序列，这对于实验和结果的可重复性至关重要。\n",
    "常见的随机操作包括：\n",
    "+ 数据集的随机拆分(如训练集和测试集的划分)\n",
    "+ 模型参数的随机初始化\n",
    "+ 数据增强中的随机变换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of setting random seed for reproducibility\n",
    "# the results will be the same each time\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "seed = 66\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "# torch.manual_seed(seed)\n",
    "random_matrix_first = np.random.randn(3, 3)\n",
    "print(random_matrix_first)\n",
    "random_data_one_first = random.random()\n",
    "random_data_two_first = random.random()\n",
    "print(random_data_one_first, random_data_two_first)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of not setting random seed\n",
    "# the results will be different each time\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "random_matrix_second = np.random.randn(3, 3)\n",
    "print(random_matrix_second)\n",
    "random_data_one_second = random.random()\n",
    "random_data_two_second = random.random()\n",
    "print(random_data_one_second, random_data_two_second)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# argparser\n",
    "是python标准库中的一个模块，用于解析命令行参数。argparser模块使得编写用户友好的命令行界面变得容易。它可以解析命令行参数并生成帮助信息。argparser模块还可以自动生成帮助信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "import argparse\n",
    "# create the object of ArgumentParser\n",
    "parser = argparse.ArgumentParser(description = \"Description of the program\")\n",
    "# add_argument() method is used to add the arguments\n",
    "parser.add_argument(\"--arg1\", help = \"Description of the arg1\")\n",
    "parser.add_argument(\"--optional_arg\", default = \"default_value\", help = \"Description of the optional_arg\")\n",
    "# parse_args() method is used to parse the arguments\n",
    "args = parser.parse_args()\n",
    "# use the arguments\n",
    "print(args.arg1)\n",
    "print(args.optional_arg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function \"DataLoader\" in torch.utils.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The parameter \"collate_fn\" of function \"DataLoader\" in torch.utils.data\n",
    "collate_fn：将一个batch的数据样本组织成一个batch，这个batch中的每个数据样本的形状可能不一样，所以需要用一个函数来组织这个batch。默认的collate_fn函数会将数据样本组织成一个list，这样就不会出现形状不一样的问题。如果数据样本的形状一样，可以不用设置collate_fn参数。\n",
    "\n",
    "作用：\n",
    "+ 自定义合并逻辑：当数据集中的样本结构复杂或者每个样本的形状不一致时，通过\"collate_fn\"自定义合并逻辑能够更灵活地处理这些样本。\n",
    "+ 处理变长序列：对于自然语言处理等任务，样本（例如句子）长度不一，需要自定义合并逻辑进行填充和打包。\n",
    "+ 处理多模态数据：当一个样本包含多种不同模态的数据（如图像和文本）时，需要自定义合并逻辑以正确处理和打包这些数据。\n",
    "\n",
    "使用方法：\n",
    "+ 定义一个函数，该函数接受一个batch的数据样本列表，返回一个batch的数据样本。'collate_fn'函数中的'batch'是一个列表，包含了多个通过'\\_\\_getitem_\\_'方法获取的数据样本。\n",
    "+ 将该函数传递给DataLoader的collate_fn参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "data = [\n",
    "    {'input': torch.tensor([1, 2, 3]), 'label': torch.tensor(0)},\n",
    "    {'input': torch.tensor([4, 5]), 'label': torch.tensor(1)},\n",
    "    {'input': torch.tensor([6, 7, 8, 9]), 'label': torch.tensor(2)}\n",
    "]\n",
    "\n",
    "dataset = CustomDataset(data)\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    inputs = [item['input'] for item in batch]\n",
    "    labels = [item['label'] for item in batch]\n",
    "\n",
    "    lengths = [len(item) for item in inputs]\n",
    "    max_length = max(lengths)\n",
    "\n",
    "    padded_inputs = torch.zeros((len(inputs), max_length))\n",
    "    for i, input in enumerate(inputs):\n",
    "        padded_inputs[i, :len(input)] = input\n",
    "    \n",
    "    labels = torch.stack(labels)\n",
    "\n",
    "    return padded_inputs, labels, lengths\n",
    "\"\"\"\n",
    "first batch:\n",
    "[\n",
    "    {'input': torch.tensor([1, 2, 3]), 'label': torch.tensor(0)},\n",
    "    {'input': torch.tensor([4, 5]), 'label': torch.tensor(1)}\n",
    "]\n",
    "\n",
    "second batch:\n",
    "[\n",
    "    {'input': torch.tensor([6, 7, 8, 9]), 'label': torch.tensor(2)}   \n",
    "]\n",
    "\"\"\"\n",
    "dataloader = DataLoader(dataset, batch_size = 2, shuffle = False, collate_fn = custom_collate_fn)\n",
    "\n",
    "for batch in dataloader:\n",
    "    padded_inputs, labels, lengths = batch\n",
    "    print(\"Padded inputs:\\n\", padded_inputs)\n",
    "    print(\"Labels:\\n\", labels)\n",
    "    print(\"Lengths:\\n\", lengths)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The parameter \"sampler\" and \"batch_sampler\" of function \"DataLoader\" in torch.utils.data\n",
    "sampler: \n",
    "\n",
    "定义如何从数据集中采样单个样本。它接受一个\"sampler\"对象。pytorch中提供了一些内置的采样器:torch.utils.data.RandomSampler, torch.utils.data.SequentialSampler, torch.utils.data.SubsetRandomSampler, torch.utils.data.WeightedRandomSampler等。这些采样器，都接收一个dataset对象，返回一个索引序列，用于获取数据集中的样本。\n",
    "\n",
    "batch_sampler: \n",
    "\n",
    "将单个样本采样器生成的样本索引打包成批次。它接受一个\"batch_sampler\"对象。pytorch中提供了一些内置的batch采样器:torch.utils.data.BatchSampler, torch.utils.data.WeightedRandomSampler等。这些batch采样器，接收一个sampler对象，返回一个索引序列，用于获取数据集中的样本。\n",
    "\n",
    "通过这两个参数，pytorch的'DataLoader'提供了灵活且强大的数据加载和采样机制，满足不同的实验和训练要求。另外可以，**自定义'sampler'和'batch_sampler'可以满足特定的数据采样需求**。\n",
    "\n",
    "自定义'sampler'：\n",
    "+ 继承torch.utils.data.Sampler类，实现\\_\\_iter\\_\\_和\\_\\_len\\_\\_方法。\n",
    "\n",
    "自定义'batch_sampler'：\n",
    "+ 继承torch.utils.data.BatchSampler类，实现\\_\\_iter\\_\\_和\\_\\_len\\_\\_方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example: sampler\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "\n",
    "data = torch.arange(10).view(-1, 1)\n",
    "dataset = TensorDataset(data)\n",
    "\n",
    "print(\"Example of sampler\")\n",
    "random_sampler = torch.utils.data.RandomSampler(dataset)\n",
    "dataloader_one = DataLoader(dataset, sampler = random_sampler, batch_size = 2)\n",
    "for batch in dataloader_one:\n",
    "    print(batch)\n",
    "\n",
    "print('-------------------')\n",
    "\n",
    "# example: batch_sampler\n",
    "print(\"Example of batch sampler\")\n",
    "batch_sampler = torch.utils.data.BatchSampler(random_sampler, batch_size = 3, drop_last = False)\n",
    "dataloader_two = DataLoader(dataset, batch_sampler = batch_sampler)\n",
    "for batch in dataloader_two:\n",
    "    print(batch)\n",
    "\n",
    "print('-------------------')\n",
    "\n",
    "# example: customize sampler\n",
    "class ReverseSampler(torch.utils.data.Sampler):\n",
    "    def __init__(self, data_source):\n",
    "        self.data_source = data_source\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return iter(range(len(self.data_source) - 1, -1, -1))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_source)\n",
    "\n",
    "print(\"Exampler of customized sampler\")\n",
    "reverse_sampler = ReverseSampler(dataset)\n",
    "dataloader_three = DataLoader(dataset, sampler = reverse_sampler, batch_size = 2) \n",
    "\n",
    "for batch in dataloader_three:\n",
    "    print(batch)\n",
    "\n",
    "print('-------------------')\n",
    "\n",
    "# example: customize batch sampler\n",
    "class CustomBatchSampler(torch.utils.data.BatchSampler):\n",
    "    def __init__(self, sampler, batch_size, drop_last):\n",
    "        self.sampler = sampler\n",
    "        self.batch_size = batch_size\n",
    "        self.drop_last = drop_last\n",
    "    \n",
    "    def __iter__(self):\n",
    "        batch = []\n",
    "        for index in self.sampler:\n",
    "            batch.append(index)\n",
    "            if len(batch) == self.batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        \n",
    "        if len(batch) > 0 and not self.drop_last:\n",
    "            yield batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.drop_last:\n",
    "            return len(self.sampler) // self.batch_size\n",
    "        else: \n",
    "            return (len(self.sampler) + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "batch_sampler = CustomBatchSampler(reverse_sampler, batch_size = 3, drop_last = False)\n",
    "dataloader_four = DataLoader(dataset, batch_sampler = batch_sampler)\n",
    "\n",
    "for batch in dataloader_four:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xavier 初始化\n",
    "Xavier初始化是用于初始化神经网络权重的一种方法，其目的是让信号能够更好地在网络层间传播，从而避免梯度消失或者爆炸的问题。\n",
    "\n",
    "Xavier初始化分为均匀分布(Xavier Uniform)和正态分布(Xavier Normal)两种方式。\n",
    "\n",
    "**Xavier Uniform**:\n",
    "\n",
    "W ~ U($-\\sqrt{\\frac{6}{n_{in}+n_{out}}}, \\sqrt{\\frac{6}{n_{in}+n_{out}}}$)\n",
    "+ U(a, b)表示均匀分布，a和b分别是分布的上下界。\n",
    "+ $n_{in}$是输入单元的数量，$n_{out}$是输出单元的数量。\n",
    "\n",
    "**Xavier Normal**:\n",
    "\n",
    "W ~ N(0, $\\sqrt{\\frac{2}{n_{in}+n_{out}}}$)\n",
    "+ N($\\mu$, $\\sigma^2$)表示正态分布，$\\mu$和$\\sigma^2$分别是分布的均值和方差。\n",
    "+ $n_{in}$是输入单元的数量，$n_{out}$是输出单元的数量。\n",
    "\n",
    "总结：\n",
    "+ Xavier Uniform：权重从均匀分布中采样，适用于浅层网络，能够提供平滑的信号传播\n",
    "+ Xavier Normal： 权重从正态分布中采样，适用于深层网络，能防止梯度消失或爆炸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "# Example: Xavier Uniform Initialization\n",
    "linear = nn.Linear(3, 2)\n",
    "nn.init.xavier_uniform_(linear.weight).requires_grad_(False)\n",
    "print(linear.weight)\n",
    "\n",
    "# Example: Xavier Normal Initialization\n",
    "linear = nn.Linear(3, 2)\n",
    "nn.init.xavier_normal_(linear.weight).requires_grad_(False)\n",
    "print(linear.weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.autograd\n",
    "https://pytorch.org/docs/stable/autograd.html\n",
    "## 自定义的前向和反向向传播函数\n",
    "方法：\n",
    "+ 继承torch.autograd.Function类\n",
    "+ 实现forward和backward方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Function\n",
    "\n",
    "class MyFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)        # 上下文对象，用于在前向传播中保存信息，以便在后向传播中使用\n",
    "        return input * 2\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors          # 从上下文对象中获取保存的信息\n",
    "        print(\"ctx saved tensor: \", ctx.saved_tensors)\n",
    "        print(\"grad output: \", grad_output)\n",
    "        grad_input = grad_output * 2        # grad_output是output.sum()的梯度，即一个标量1。根据链式法则，input的梯度是grad_output * 2\n",
    "        print(\"gard input: \", grad_input)\n",
    "        return grad_input\n",
    "\n",
    "input = torch.tensor([1.0, 2.0, 3.0], requires_grad = True)\n",
    "output = MyFunction.apply(input)            # apply()方法用于调用自定义的Function; 类中定义的静态方法不需要实例化即可调用\n",
    "print(\"output: \", output)\n",
    "output.sum().backward()                     # 从标量\"output_sum\"出发，计算计算图反向传播，计算每个叶子节点的梯度\n",
    "\n",
    "print(\"input.grad: \", input.grad)\n",
    "\n",
    "a = torch.tensor(1.0, requires_grad = True)\n",
    "b = torch.tensor(2.0, requires_grad = True)\n",
    "c = torch.tensor(3.0, requires_grad = True)\n",
    "f1 = a + b\n",
    "f2 = b + c\n",
    "output = f1 * f2\n",
    "output.backward()\n",
    "print(a.grad, b.grad, c.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## @once_differentiable\n",
    "@once_differentiable 是pytorch中的一个装饰器，主要用于装饰自定义的自动求导函数。这个装饰器主要用于确保编写的自定义函数是可微的(differentiable)，并且仅在一次微分时有效。这是为了增强梯度计算的稳定性和准确性。\n",
    "\n",
    "具体来说，'@once_differentiable\"用于装饰那些实现了\"torch.autograd.Function\"类的自定义函数。这些自定义函数需要实现\"forward\"和\"backward\"方法，其中\"forward\"方法定义了前向传播和计算，\"backward\"方法定义了反向传播时的梯度计算。\n",
    "\n",
    "使用这个装饰器，可以确保自定义函数在反向传播时能够正确地计算梯度，避免由于函数不可微导致的梯度计算问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input.grad:  tensor([2., 4., 6.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Function\n",
    "from torch.autograd.function import once_differentiable\n",
    "\n",
    "class MyFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        return input ** 2\n",
    "\n",
    "    @staticmethod\n",
    "    @once_differentiable   \n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        # 返回输入的梯度\n",
    "        grad_input = 2 * input * grad_output\n",
    "        return grad_input\n",
    "\n",
    "input = torch.tensor([1.0, 2.0, 3.0], dtype = torch.float32, requires_grad = True)\n",
    "output = MyFunction.apply(input)\n",
    "output.sum().backward()\n",
    "print(\"input.grad: \", input.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data create & process in pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 维度转换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.flatten()\n",
    "\n",
    "\"flatten\"方法用于将张量的多个维度展平成一个维度。这个方法特别适用于将高维数据转换为一维数据。\n",
    "\n",
    "调用方法：torch.flatten(input, start_dim=0, end_dim=-1) -> Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before:  torch.Size([3, 4, 5, 2, 9])\n",
      "after:  torch.Size([3, 4, 10, 9])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.randn((3, 4, 5, 2, 9))\n",
    "print(\"before: \", data.shape)\n",
    "\n",
    "data_flatten = data.flatten(start_dim = 2, end_dim = 3)\n",
    "print(\"after: \", data_flatten.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.transpose()\n",
    "\"transpose\"方法用于交换张量的两个维度。它只能同时交换两个维度，而不能处理超过两个维度的交换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before:  torch.Size([3, 4, 8, 9])\n",
      "after:  torch.Size([8, 4, 3, 9])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.randn((3, 4, 8, 9))\n",
    "print(\"before: \", data.shape)\n",
    "\n",
    "data_transpose = data.transpose(0, 2)       # the same as data.transpose(2, 0)\n",
    "print(\"after: \", data_transpose.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.permute()\n",
    "\"permute\"方法用于根据指定的新维度顺序重排张量的所有维度。它允许任意数量的维度重排。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before:  torch.Size([3, 2, 4, 5])\n",
      "after:  torch.Size([5, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.randn((3, 2, 4, 5))\n",
    "print(\"before: \", data.shape)\n",
    "\n",
    "data_permute = data.permute(3, 1, 0, 2)\n",
    "print(\"after: \", data_permute.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.view()\n",
    "\"torch.view\"方法用于在不改变数据内容的前提下，重新调整张量的形状。\"view\"是基于原始张量创建一个新张量，这两个张量共享相同的数据存储，因此，修改其中一个张量的内容会影响到另一个张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before:  torch.Size([3, 4, 5])\n",
      "after:  torch.Size([3, 20])\n",
      "tensor([[[1., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.zeros((3, 4, 5))\n",
    "print(\"before: \", data.shape)\n",
    "\n",
    "data_view = data.view(3, -1)\n",
    "print(\"after: \", data_view.shape)\n",
    "\n",
    "data_view[0, 0] = 1     # 同时改变了data的值\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 维度拼接"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.cat()\n",
    "\"cat\"方法用于将一组张量沿指定维度拼接起来。torch.cat(tensors, dim=0) -> Tensor\n",
    "+ 被拼接的张量必须在除了指定维度外的所有维度形状相同\n",
    "+ 拼接后的张量在指定维度上的大小是所有输入张量在该维度上的大小之和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after:  torch.Size([3, 80, 4])\n",
      "after:  torch.Size([10, 5, 109])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data_one = torch.randn((3, 20, 4))\n",
    "data_two = torch.randn((3, 60, 4))\n",
    "\n",
    "data_cat_first = torch.cat([data_one, data_two], dim = 1)\n",
    "print(\"after: \", data_cat_first.shape)\n",
    "\n",
    "data_three = torch.randn((10, 5, 9))\n",
    "data_four = torch.randn((10, 5, 100))\n",
    "data_cat_second = torch.cat([data_three, data_four], dim = 2)\n",
    "print(\"after: \", data_cat_second.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.stack()\n",
    "\"stack\"方法用于将一组张量沿新维度拼接起来。torch.stack(tensors, dim=0) -> Tensor\n",
    "+ 被拼接的张量必须在所有维度上形状相同\n",
    "+ 堆叠后的张量会增加一个新维度，这个新维度的大小等于输入张量的数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after:  torch.Size([3, 4, 5])\n",
      "tensor([[[ 0.5067, -1.2020,  1.8099, -0.4309,  0.0798],\n",
      "         [ 0.6174,  0.8297,  0.8262, -0.4378,  1.4239],\n",
      "         [ 0.6762,  1.5456,  1.0412,  2.4749,  0.0731],\n",
      "         [ 1.6772, -0.2456, -1.4661,  0.7084, -0.6128]],\n",
      "\n",
      "        [[ 1.2085,  0.4101,  0.0168, -0.0517,  1.0677],\n",
      "         [-1.6646,  1.4566,  1.0326,  0.6308, -1.3728],\n",
      "         [-0.2930, -0.4203,  1.2512, -0.1092,  0.1509],\n",
      "         [ 0.6968,  0.4492, -0.8958, -0.5760, -0.4606]],\n",
      "\n",
      "        [[ 1.4241,  0.6967, -1.1816, -0.4726,  1.2660],\n",
      "         [-0.0825, -0.2198, -1.4862, -0.9830, -0.0110],\n",
      "         [ 0.4215,  0.8529, -0.0222,  0.1961,  0.3469],\n",
      "         [-0.3362,  0.0602, -1.3227, -0.9573, -0.3047]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data_one = torch.randn((3, 4))\n",
    "data_two = torch.randn((3, 4))\n",
    "data_three = torch.randn((3, 4))\n",
    "data_four = torch.randn((3, 4))\n",
    "data_five = torch.randn((3, 4))\n",
    "\n",
    "data_stack = torch.stack([data_one, data_two, data_three, data_four, data_five], dim = 2)      # 参数dim的值不能超过堆叠后的数据维度，例如堆叠后的维度数为3， 那么dim的值不能超过2\n",
    "print(\"after: \", data_stack.shape)\n",
    "print(data_stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 按维度计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.prod()\n",
    "\n",
    "\"prod\"方法用于计算张量的所有元素或指定维度上的元素的乘积。这个方法在处理需要计算累积乘积的任务时非常重要。\n",
    "\n",
    "调用方法：torch.prod(input, dim = None, keepdim = False, dype = None) -> Tensor; 若未指定dim，则计算所有元素的乘积。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[2, 3, 4],\n",
      "         [5, 6, 7]],\n",
      "\n",
      "        [[1, 2, 3],\n",
      "         [4, 5, 6]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[10, 18, 28],\n",
      "        [ 4, 10, 18]])\n",
      "torch.Size([2, 3])\n",
      "tensor([[[10, 18, 28]],\n",
      "\n",
      "        [[ 4, 10, 18]]])\n",
      "torch.Size([2, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.tensor([\n",
    "        [[2, 3, 4], [5, 6, 7]],\n",
    "        [[1, 2, 3], [4, 5, 6]]\n",
    "    ])\n",
    "print(data)\n",
    "print(data.shape)\n",
    "print(data.prod(1))\n",
    "print(data.prod(1).shape)\n",
    "print(data.prod(1, keepdim = True))\n",
    "print(data.prod(1, keepdim = True).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.cumsum()\n",
    "\n",
    "\"cumsum\"方法用于计算张量沿指定维度的累计和(cumulative sum)。该方法返回一个张量，其元素是沿给定维度的累积和。**累积和的结果会与原始张量具有相同形状**。\n",
    "\n",
    "调用方法：torch.cumsum(input, dim, dtype = None) -> Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before data: \n",
      " tensor([[[2, 3, 4],\n",
      "         [5, 6, 7]],\n",
      "\n",
      "        [[1, 2, 3],\n",
      "         [4, 5, 6]]])\n",
      "before data.shape: \n",
      " torch.Size([2, 2, 3])\n",
      "after data_cumsum_dim1: \n",
      " tensor([[[ 2,  3,  4],\n",
      "         [ 7,  9, 11]],\n",
      "\n",
      "        [[ 1,  2,  3],\n",
      "         [ 5,  7,  9]]])\n",
      "after data_cumsum_dim1.shape: \n",
      " torch.Size([2, 2, 3])\n",
      "after data_cumsum_dim2: \n",
      " tensor([[[ 2,  5,  9],\n",
      "         [ 5, 11, 18]],\n",
      "\n",
      "        [[ 1,  3,  6],\n",
      "         [ 4,  9, 15]]])\n",
      "after data_cumsum_dim2.shape: \n",
      " torch.Size([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.tensor([\n",
    "        [[2, 3, 4], [5, 6, 7]],\n",
    "        [[1, 2, 3], [4, 5, 6]]\n",
    "    ])\n",
    "print(\"before data: \\n\", data)\n",
    "print(\"before data.shape: \\n\", data.shape)\n",
    "\n",
    "data_cumsum_dim1 = data.cumsum(dim = 1)\n",
    "data_cumsum_dim2 = data.cumsum(dim = 2)\n",
    "print(\"after data_cumsum_dim1: \\n\", data_cumsum_dim1)\n",
    "print(\"after data_cumsum_dim1.shape: \\n\", data_cumsum_dim1.shape)\n",
    "print(\"after data_cumsum_dim2: \\n\", data_cumsum_dim2)\n",
    "print(\"after data_cumsum_dim2.shape: \\n\", data_cumsum_dim2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 转换为张量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.tensor()\n",
    "将数据转换为张量。torch.tensor(data, dtype=None, device=None, requires_grad=False) -> Tensor\n",
    "\n",
    "特点：**总是会创建数据的副本**，即使输入数据已经是一个张量或者是一个numpy数组。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list:  [1, 2, 3, 4]\n",
      "numpy:  [1 2 3 4]\n",
      "tensor:  tensor([1, 2, 3, 4])\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "data_list = [1, 2, 3, 4]\n",
    "data_numpy = np.array(data_list)\n",
    "data_tensor = torch.tensor(data_numpy)\n",
    "print(\"list: \", data_list)\n",
    "print(\"numpy: \", data_numpy)\n",
    "print(\"tensor: \", data_tensor)\n",
    "print(data_numpy.ctypes.data == data_tensor.data_ptr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.as_tensor()\n",
    "将数据转化成张量。如果输入数据已经是一个张量或者是一个numpy数组，则可以共享内存，不会进行数据复制。**它会尽量与输入数据共享内存**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list:  [1, 2, 3, 4]\n",
      "numpy:  [1 2 3 4]\n",
      "tensor:  tensor([1, 2, 3, 4])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "data_list = [1, 2, 3, 4]\n",
    "data_numpy = np.array(data_list)\n",
    "data_as_tensor = torch.as_tensor(data_numpy)\n",
    "print(\"list: \", data_list)\n",
    "print(\"numpy: \", data_numpy)\n",
    "print(\"tensor: \", data_as_tensor)\n",
    "print(data_numpy.ctypes.data == data_as_tensor.data_ptr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 掩码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensor.masked_fill()\n",
    "\"mask_fill\"方法用于根据掩码(mask)将张量中指定位置的元素替换为一个指定的值。掩码通常是一个布尔张量或与原张量形状相同的二进制张量，其中'True'或'1'的位置表示需要替换的位置。\n",
    "\n",
    "调用方法: tensor.masked_fill(mask, value) -> Tensor\n",
    "+ mask: 一个布尔张量或与原张量形状相同的二进制张量。'mask'中为'True'或'1'的位置会被'value'替换\n",
    "+ value:用于替换掩码中指定位置的值\n",
    "\n",
    "输出结果与原张量具有相同的形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:  tensor([[[ 1.2971, -0.1103],\n",
      "         [ 0.5934,  0.9164],\n",
      "         [ 0.3821,  0.7289],\n",
      "         [ 0.0761, -1.7662],\n",
      "         [-0.1889,  1.0144],\n",
      "         [ 0.2463, -0.9795]],\n",
      "\n",
      "        [[-1.1312, -1.4169],\n",
      "         [ 0.2797, -0.3052],\n",
      "         [-1.1987, -0.4208],\n",
      "         [ 0.4150, -0.2298],\n",
      "         [ 0.4249,  1.3761],\n",
      "         [-0.3598, -0.2692]]])\n",
      "data.shape:  torch.Size([2, 6, 2])\n",
      "mask's shape:  torch.Size([2, 6, 1])\n",
      "data_masked:  tensor([[[ 0.0000,  0.0000],\n",
      "         [ 0.5934,  0.9164],\n",
      "         [ 0.0000,  0.0000],\n",
      "         [ 0.0761, -1.7662],\n",
      "         [ 0.0000,  0.0000],\n",
      "         [ 0.2463, -0.9795]],\n",
      "\n",
      "        [[-1.1312, -1.4169],\n",
      "         [ 0.0000,  0.0000],\n",
      "         [-1.1987, -0.4208],\n",
      "         [ 0.0000,  0.0000],\n",
      "         [ 0.4249,  1.3761],\n",
      "         [ 0.0000,  0.0000]]])\n",
      "data_masked.shape:  torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.randn((2, 6, 2))\n",
    "print(\"data: \", data)\n",
    "print(\"data.shape: \", data.shape)\n",
    "mask = torch.tensor([\n",
    "    [True, False, True, False, True, False],\n",
    "    [False, True, False, True, False, True]\n",
    "])\n",
    "print(\"mask's shape: \", mask[..., None].shape)          # mask[..., None] is equivalent to mask[:, :, None]\n",
    "data_masked = data.masked_fill(mask[..., None], 0)\n",
    "print(\"data_masked: \", data_masked)\n",
    "print(\"data_masked.shape: \", data_masked.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立特殊值张量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.zeros()\n",
    "\n",
    "\"zeros\"方法用于创建一个全为0的张量。torch.zeros(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "size = (3, 4)\n",
    "data = torch.zeros(size, dtype = torch.float32)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensor.new_zeros()\n",
    "\n",
    "\"tensor.new_zeros\"方法用于创建一个与给定张量具有相同数据类型和设备的全零张量。\n",
    "\n",
    "特点：\n",
    "+ 新创建的张量继承了调用张量的dtype 和 device属性\n",
    "+ 不需要显式指定dtype 和 device，除非需要覆盖调用张量的dtype 和 device属性\n",
    "\n",
    "调用方法：tensor.new_zeros(size, dtype = None, device = None, requires_grad = False) -> Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5]) torch.float32 cpu\n",
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.randn((3, 4), dtype = torch.float32, device = torch.device('cpu'))\n",
    "\n",
    "data_as_zeros = data.new_zeros((2, 5))\n",
    "print(data_as_zeros.shape, data_as_zeros.dtype, data_as_zeros.device)\n",
    "print(data_as_zeros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.zeros_like()\n",
    "\n",
    "\"torch.zeros_like\"方法用于创建一个与给定张量具有相同形状的全为0的张量。\n",
    "\n",
    "调用方法：torch.zeros_like(input, dtype=None, layout=None, device=None, requires_grad=False) -> Tensor\n",
    "\n",
    "与\"tensor.new_zeros\"方法的区别：\n",
    "+ \"tensor.new_zeros\"方法创建的张量与调用张量具有相同的dtype和device属性, 尽管可以覆盖这些属性。但是需要显式给出形状(size)参数\n",
    "+ \"torch.zeros_like\"方法创建的张量与给定张量具有相同的形状, 不需要显式给出形状参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: \n",
      " tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n",
      "data_zeros_like: \n",
      " tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.zeros((3, 4), dtype = torch.float32)\n",
    "data_zeros_like = torch.zeros_like(data)\n",
    "print(\"data: \\n\", data)\n",
    "print(\"data_zeros_like: \\n\", data_zeros_like)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
